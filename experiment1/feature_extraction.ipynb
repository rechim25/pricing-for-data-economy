{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_refs = np.load('selected_datasets_ref.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rabemampiandraeric_currency'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [ref.replace('/', '_') for ref in selected_refs]\n",
    "folders[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Checking Downloaded Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find refs of missing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 7 datasets: ['ajaxfb_prodemg-services', 'jackvial_themaestrodatasetv2', 'sandhaya4u_histology-image-dataset', 'williamkaiser_all-isef-projects', 'roydatascience_chemistry-models', 'wjhyde1_2014-acs-dashboard', 'artemnaida_canadian-economic-data']\n"
     ]
    }
   ],
   "source": [
    "missing_folders = []\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        missing_folders.append(folder)\n",
    "print('Missing %d datasets: %s' % (len(missing_folders), missing_folders))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove refs of missing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove 7 folder names\n"
     ]
    }
   ],
   "source": [
    "size_before = len(folders)\n",
    "for missing_folder in missing_folders:\n",
    "    folders.remove(missing_folder)\n",
    "print('Remove %d folder names' % (size_before - len(folders)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all datasets folders contain only files or have nested directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136 folders with nonfiles (directories)\n"
     ]
    }
   ],
   "source": [
    "folder_with_nonfiles = []\n",
    "for folder in folders:\n",
    "    folder_items = os.listdir(folder)\n",
    "    for item in folder_items:\n",
    "        if not os.path.isfile(os.path.join(folder, item)):\n",
    "            folder_with_nonfiles.append(folder)\n",
    "            break\n",
    "print('Found %d folders with nonfiles (directories)' % len(folder_with_nonfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hunhthanhphong_health-news-in-twitter-uci\n",
      "['Health-Tweets']\n",
      "\n",
      "hariwh0_vietnam-stock-competition\n",
      "['modules']\n",
      "\n",
      "ayushranjan1610_movies\n",
      "['.ipynb_checkpoints']\n",
      "\n",
      "husnantaj_review-scrapper\n",
      "['.idea', 'static', 'templates']\n",
      "\n",
      "dataup1_ogbg-moltox21\n",
      "['scaffold']\n",
      "\n",
      "sgreiner_arc-rasp\n",
      "['output']\n",
      "\n",
      "vin1234_detecting-sentiments-of-a-quote\n",
      "['Dataset', 'Sample Data Files']\n",
      "\n",
      "evanregan_dsci-511-us-county-health-dataset\n",
      "['pickles']\n",
      "\n",
      "sripaadsrinivasan_prokabadiseassion17stats\n",
      "['Data']\n",
      "\n",
      "xenophule_udemy-class-pca\n",
      "['PCA']\n",
      "\n",
      "rafaelgreca_wnba-games-box-score-since-1997\n",
      "['Data']\n",
      "\n",
      "brettlines_bbc-scotland-house-party-data\n",
      "['paul']\n",
      "\n",
      "ryuuseikuhome_azuralane-ships-data\n",
      "['blhx_avatar']\n",
      "\n",
      "tabarkarajab_decktrot-cards-dataset\n",
      "['test dataset', 'train dataset']\n",
      "\n",
      "bangdaeho_korea-money\n",
      "['images', 'labels']\n",
      "\n",
      "teguhpermana_data-analysis-gdp-per-capita-usd\n",
      "['latihan1']\n",
      "\n",
      "teguhpermana_data-analysis-gdp-per-capita-usd\n",
      "['lifesat']\n",
      "\n",
      "teguhpermana_data-analysis-gdp-per-capita-usd\n",
      "['fundamentals']\n",
      "\n",
      "irwansyah10010_mobilelegend\n",
      "['Atribut', 'Skill']\n",
      "\n",
      "visualcomments_all-distil-features\n",
      "['all_distil_features']\n",
      "\n",
      "visualcomments_all-distil-features\n",
      "['features']\n",
      "\n",
      "zq1200_hans-niemann-chess-games\n",
      "['csv']\n",
      "\n",
      "zq1200_hans-niemann-chess-games\n",
      "['pgn']\n",
      "\n",
      "aadamg_skyrim-books-from-uesp\n",
      "['.ipynb_checkpoints']\n",
      "\n",
      "shagalsajid_call-of-duty-mobile-weapons-stats\n",
      "['images']\n",
      "\n",
      "mhkoosheshi_asdfmri\n",
      "['ASD', 'Normal']\n",
      "\n",
      "zq1200_gary-kasparov-complete-chess-games-19762021\n",
      "['csv']\n",
      "\n",
      "zq1200_gary-kasparov-complete-chess-games-19762021\n",
      "['pgn']\n",
      "\n",
      "shawnwuplus_drone-trajectory-data\n",
      "['IncludeTakeoff', 'WithoutTakeoff']\n",
      "\n",
      "rkuo2000_coronavirus-genome-data\n",
      "['index']\n",
      "\n",
      "zq1200_mikhail-tal-complete-chess-games\n",
      "['csv']\n",
      "\n",
      "zq1200_mikhail-tal-complete-chess-games\n",
      "['pgn']\n",
      "\n",
      "victorfernandezalbor_kagglefold\n",
      "['beta', 'kagglefold', 'TemplateServer', 'test-data', 'tests', 'utils']\n",
      "\n",
      "shankarramharack_10km-cg-de\n",
      "['Fault Simulations']\n",
      "\n",
      "ksj97207_noaa-fisheries-steller-sea-lion-population-count\n",
      "['dataset', 'img', 'source']\n",
      "\n",
      "figurita2_fifa-ultimate-team-db\n",
      "['GKs']\n",
      "\n",
      "figurita2_fifa-ultimate-team-db\n",
      "['Players']\n",
      "\n",
      "zq1200_2022-superunited-chess-rapid-blitz-croatia\n",
      "['csv']\n",
      "\n",
      "zq1200_2022-superunited-chess-rapid-blitz-croatia\n",
      "['pgn']\n",
      "\n",
      "anandkumarsahu09_ipl-player-stats-20162022\n",
      "['Batting Stats', 'Bowling Stats']\n",
      "\n",
      "knightnikhil_cardefect\n",
      "['test', 'train']\n",
      "\n",
      "nnjjpp_1000-random-mazes\n",
      "['.git']\n",
      "\n",
      "zq1200_2022-sinquefield-cup-grand-chess-tour\n",
      "['csv']\n",
      "\n",
      "zq1200_2022-sinquefield-cup-grand-chess-tour\n",
      "['pgn']\n",
      "\n",
      "dschettler8845_sampdi3d\n",
      "['nucleic', 'protein']\n",
      "\n",
      "jagan028_mlware-preprocessed\n",
      "['Faces_test', 'Faces_train', 'Landmarks_test', 'Landmarks_train']\n",
      "\n",
      "jagan028_mlware-preprocessed\n",
      "['Demo images']\n",
      "\n",
      "aiizloli_pytorchimagemodelsmaster\n",
      "['ISSUE_TEMPLATE', 'workflows']\n",
      "\n",
      "aiizloli_pytorchimagemodelsmaster\n",
      "['javascripts', 'models']\n",
      "\n",
      "aiizloli_pytorchimagemodelsmaster\n",
      "['data', 'loss', 'models', 'optim', 'scheduler', 'utils']\n",
      "\n",
      "mateuscco_brazilian-aircraft-fleet\n",
      "['RawData']\n",
      "\n",
      "zq1200_bobby-fischer-complete-chess-games-1955-1992\n",
      "['csv']\n",
      "\n",
      "zq1200_bobby-fischer-complete-chess-games-1955-1992\n",
      "['pgn']\n",
      "\n",
      "kittlein_birdclef-probability-of-species-occurrence\n",
      "['av']\n",
      "\n",
      "meetnagadia_share-price-of-top-electric-car-company\n",
      "['Audi', 'BMW', 'Honda', 'Lucid Motors', 'NIO', 'Nissan', 'Rolls Royces', 'Tata', 'Tesla', 'Volkswagen']\n",
      "\n",
      "iitsnotme_pokemon\n",
      "['images']\n",
      "\n",
      "taupiphi_league-of-legends-voice-lines\n",
      "['.ipynb_checkpoints']\n",
      "\n",
      "usmanbasharat_predicting-a-car-price-of-car-connection-picture\n",
      "['SourceCode']\n",
      "\n",
      "sapal6_bird-speciestiny\n",
      "['BANDED BROADBILL', 'BIRD OF PARADISE']\n",
      "\n",
      "sapal6_bird-speciestiny\n",
      "['BANDED BROADBILL', 'BIRD OF PARADISE']\n",
      "\n",
      "sapal6_bird-speciestiny\n",
      "['BANDED BROADBILL', 'BIRD OF PARADISE']\n",
      "\n",
      "brandonqilin_arcane-soundtrack-lyrics\n",
      "['acts', 'songs']\n",
      "\n",
      "csmalarkodi_recovery-covid19-news-credibility-research\n",
      "['reliable', 'unreliable']\n",
      "\n",
      "dataup1_ogbg-molhiv\n",
      "['scaffold']\n",
      "\n",
      "mishalleni_machinelearning\n",
      "['τ¼¼1Φ»╛ µ£║σÖ¿σ¡ªΣ╣áσ«₧µêÿ', 'τ¼¼5Φ»╛ µ╖▒σ║ªτÑ₧τ╗Åτ╜æτ╗£']\n",
      "\n",
      "shivkp_hackerearth-ml-love-is-love\n",
      "['Dataset', 'Sample Data Files']\n",
      "\n",
      "danilalapokin_trading\n",
      "['.ipynb_checkpoints', 'dates']\n",
      "\n",
      "zq1200_aron-nimzowitsch-chess-games-1905-to-1934\n",
      "['csv']\n",
      "\n",
      "zq1200_aron-nimzowitsch-chess-games-1905-to-1934\n",
      "['pgn']\n",
      "\n",
      "surajiiitm_bccd-dataset\n",
      "['BCCD', 'dataset', 'scripts']\n",
      "\n",
      "park123_lol-data\n",
      "['champ']\n"
     ]
    }
   ],
   "source": [
    "for folder in folder_with_nonfiles:\n",
    "    items = os.listdir(folder)\n",
    "    sub_folders = [item for item in items if os.path.isdir(os.path.join(folder, item))]\n",
    "    for sub_folder in sub_folders:\n",
    "        items = os.listdir(os.path.join(folder, sub_folder))\n",
    "        sub_sub_folders = [item for item in items if os.path.isdir(os.path.join(folder, sub_folder, item))]\n",
    "        if len(sub_sub_folders) > 0:\n",
    "            print('\\n' + folder)\n",
    "            print(sub_sub_folders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk the dataset directories in depth. For each dataset, we collect the name of all files into a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_list_files(folder_to_file_names: dict[str, list]) -> list:\n",
    "    return [val for sublist in list(folder_to_file_names.values()) for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045 folders total, 200817 files total\n"
     ]
    }
   ],
   "source": [
    "folder_to_file_names = {}\n",
    "for folder in folders:\n",
    "    folder_to_file_names[folder] = []\n",
    "    for path, subdirs, files in os.walk(folder):\n",
    "        for name in files:\n",
    "            folder_to_file_names[folder].append(os.path.join(path, name))\n",
    "print('%d folders total, %d files total' % (len(folder_to_file_names), len(get_flat_list_files(folder_to_file_names))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how may file types we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 file types\n",
      "{'', '.JPG', '.mat', '.pdf', '.model', '.hdf5', '.ffn', '.CSV', '.db', '.whl', '.jpg', '.R', '.pth', '.dat', '.xz', '.index', '.json', '.patch', '.PNG', '.bgz', '.sqlite', '.mod', '.names', '.shp', '.nex', '.fasta', '.txt', '.waypoints', '.Rtab', '.seq', '.pptx', '.xlsx', '.faa', '.iml', '.log', '.parquet', '.shx', '.geojson', '.css', '.sql', '.gb', '.fna', '.jfif', '.lua', '.prj', '.dot', '.yml', '.sav', '.in', '.fa', '.win', '.aln', '.Rd', '.sh', '.rtf', '.npy', '.go', '.arff', '.docx', '.cfg', '.py', '.a3m', '.js', '.c', '.xml', '.html', '.dbf', '.sum', '.pgn', '.yxmd', '.content', '.jpeg', '.pt', '.png', '.pickle', '.tsv', '.csv', '.sample', '.gbk', '.pkl', '.data-00000-of-00001', '.pdb', '.gbwithparts', '.emb', '.mp3', '.md', '.rds', '.gp', '.xls', '.ipynb'}\n"
     ]
    }
   ],
   "source": [
    "file_types = set()\n",
    "for folder, files in folder_to_file_names.items():\n",
    "    for file_path_str in files:\n",
    "        type = Path(file_path_str).suffix\n",
    "        file_types.add(type)\n",
    "print('%d file types' % len(file_types))  \n",
    "print(file_types)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove folders that only have models (e.g., remove husnantaj_review-scrapper as it has no data, it's just a scraper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044 folders total\n"
     ]
    }
   ],
   "source": [
    "folder_to_file_names.pop('husnantaj_review-scrapper')\n",
    "print('%d folders total' % len(folder_to_file_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, change 'paruqet' to 'parquet' manually (mispelling) ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non-data file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200357 folders total\n"
     ]
    }
   ],
   "source": [
    "# *Note*: if folder contains '.md' files most likely it is a README, Contributing, or a model file.\n",
    "types_to_remove = ['.pptx', '.lua', '.patch', '.iml', '.yml', '.ipynb', '.R', '.pdf', '.c', '.js', '.py', '.go']\n",
    "\n",
    "for folder, files in folder_to_file_names.items():\n",
    "    files_to_remove_from_folder = []\n",
    "    for file_path_str in files:\n",
    "        type = Path(file_path_str).suffix\n",
    "        if type in types_to_remove:\n",
    "            files_to_remove_from_folder.append(file_path_str)\n",
    "    if len(files_to_remove_from_folder) > 0:\n",
    "        print(\"Removing from folder %s the following files: %s\" % (folder, files_to_remove_from_folder))\n",
    "        for file_to_remove in files_to_remove_from_folder:\n",
    "            folder_to_file_names[folder].remove(file_to_remove)\n",
    "print('%d folders total' % len(get_flat_list_files(folder_to_file_names)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out any folders (datasets) that have no files left after above removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044 folders total\n"
     ]
    }
   ],
   "source": [
    "for folder, files in folder_to_file_names.items():\n",
    "    if len(files) == 0:\n",
    "        print('Removing empty folder %s' % folder)\n",
    "        folder_to_file_names.pop(folder)\n",
    "print('%d folders total' % len(folder_to_file_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (quality attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 file types remaining: {'', '.JPG', '.mat', '.model', '.hdf5', '.xls', '.ffn', '.CSV', '.db', '.whl', '.jpg', '.pth', '.dat', '.xz', '.index', '.json', '.PNG', '.bgz', '.sqlite', '.mod', '.names', '.shp', '.nex', '.fasta', '.txt', '.waypoints', '.seq', '.xlsx', '.faa', '.log', '.parquet', '.shx', '.geojson', '.css', '.sql', '.gb', '.fna', '.jfif', '.prj', '.dot', '.sav', '.in', '.fa', '.win', '.aln', '.Rd', '.sh', '.rtf', '.npy', '.go', '.arff', '.docx', '.cfg', '.a3m', '.xml', '.html', '.dbf', '.sum', '.pgn', '.yxmd', '.content', '.jpeg', '.pt', '.png', '.pickle', '.tsv', '.csv', '.sample', '.gbk', '.pkl', '.data-00000-of-00001', '.pdb', '.gbwithparts', '.emb', '.mp3', '.md', '.rds', '.gp', '.Rtab'}\n"
     ]
    }
   ],
   "source": [
    "# Remaining file types\n",
    "remaining_file_types = set()\n",
    "for folder, files in folder_to_file_names.items():\n",
    "    for file_path_str in files:\n",
    "        type = Path(file_path_str).suffix\n",
    "        remaining_file_types.add(type)\n",
    "print('%d file types remaining: %s' % (len(remaining_file_types), str(remaining_file_types)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work only with csv and parquet for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_types = ['.csv'] #'.xls', '.xlsx', '.json', '.geojson', '.CSV', '.parquet' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiler works for '.csv', '.parquet', '.avro', '.json', graph, and text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sourabhji_availabilty-of-groundwater-future-use\\\\Groundwater.csv',\n",
       " 'sourabhji_availabilty-of-groundwater-future-use\\\\mytree.dot']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_to_file_names['sourabhji_availabilty-of-groundwater-future-use']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataprofiler import Data, Profiler\n",
    "\n",
    "data = Data(\"sourabhji_availabilty-of-groundwater-future-use\\\\Groundwater.csv\") # Auto-Detect & Load: CSV, AVRO, Parquet, JSON, Text\n",
    "\n",
    "profile = Profiler(data) # Calculate Statistics, Entity Recognition, etc\n",
    "report = profile.report(report_options={\"output_format\":\"pretty\"})\n",
    "print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_stats = report['global_stats']\n",
    "data_stats = report['data_stats']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency\n",
    "\n",
    "- Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_int_features=0\n",
      "n_float_features=12\n",
      "n_string_features=2\n",
      "n_datetime_features=0\n",
      "n_categorical_features=1\n",
      "n_categorical_features=1\n",
      "is_datetime_consistent=1\n",
      "is_a_feature_ordered=1\n"
     ]
    }
   ],
   "source": [
    "# Num of integer features (int)\n",
    "n_int_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] == 'integer':\n",
    "        n_int_features += 1\n",
    "\n",
    "# Num of float features (int)\n",
    "n_float_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] == 'float':\n",
    "        n_float_features += 1\n",
    "\n",
    "# Num of string features (int)\n",
    "n_string_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] == 'string':\n",
    "        n_string_features += 1\n",
    "\n",
    "# Num of datetime features (int)\n",
    "n_datetime_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] == 'datetime':\n",
    "        n_datetime_features += 1\n",
    "\n",
    "# Num of categorical features (int)\n",
    "n_categorical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['categorical'] is True:\n",
    "        n_categorical_features += 1\n",
    "\n",
    "# Are data types consistent throughout the dataset? (one-hot) TODO: do for numerical types as well\n",
    "is_datetime_consistent = True\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] == 'datetime':\n",
    "        if 'format' in feature_stats['statistics']:\n",
    "            format_list_str = feature_stats['statistics']['format']\n",
    "            n_datetime_formats = len(format_list_str.split(','))\n",
    "            if format_list_str == '[]' or n_datetime_formats > 1:\n",
    "                is_datetime_consistent = False\n",
    "                break\n",
    "\n",
    "# At least one feature has ordered values? (one-hot)\n",
    "is_a_feature_ordered = True\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['order'] != 'random':\n",
    "        is_a_feature_ordered = True\n",
    "        break\n",
    "\n",
    "print('n_int_features=%d' % n_int_features)\n",
    "print('n_float_features=%d' % n_float_features)\n",
    "print('n_string_features=%d' % n_string_features)\n",
    "print('n_datetime_features=%d' % n_datetime_features)\n",
    "print('n_categorical_features=%d' % n_categorical_features)\n",
    "print('is_datetime_consistent=%d' % is_datetime_consistent)\n",
    "print('is_a_feature_ordered=%d' % is_a_feature_ordered)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File format extension (type)? (e.g., csv, xlsx, txt, etc.)\n",
    "# TODO\n",
    "\n",
    "\n",
    "# Structure type? (i.e., structured, semi-structured, unstructured)\n",
    "structure_to_extension = {\n",
    "    'structured': set(['csv', 'parquet', 'xls', 'xlsx', 'hdf5', 'sqlite', 'sql', 'pickle', 'pgn', ]),\n",
    "    'semi-structured': set(['xml', 'json', 'geojson', 'html', 'css']),\n",
    "    # Everything else is unstructured\n",
    "}\n",
    "# TODO\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplication and Nullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_duplicated_rows=0\n",
      "has_null_rows=0\n",
      "has_null_feature_values=0\n"
     ]
    }
   ],
   "source": [
    "# Contains duplicated rows?\n",
    "has_duplicated_rows = 0 if global_stats['duplicate_row_count'] == 0 else 1\n",
    "\n",
    "# Contains null rows?\n",
    "has_null_rows = 0 if global_stats['row_is_null_ratio'] == 0 else 1\n",
    "\n",
    "# Contains null feature values?\n",
    "has_null_feature_values = 0 if global_stats['row_has_null_ratio'] == 0 else 1\n",
    "\n",
    "print('has_duplicated_rows=%d' % has_duplicated_rows)\n",
    "print('has_null_rows=%d' % has_null_rows)\n",
    "print('has_null_feature_values=%d' % has_null_feature_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Volume and Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num of features\n",
    "n_features = global_stats['column_count']\n",
    "\n",
    "# Num rows\n",
    "n_rows = global_stats['column_count']\n",
    "\n",
    "# Num rows x num features\n",
    "n_rows_times_features = n_features * n_rows\n",
    "\n",
    "# Num bytes\n",
    "# TODO\n",
    "\n",
    "print('n_features=%d' % n_features)\n",
    "print('n_rows=%d' % n_rows)\n",
    "print('n_rows_times_features=%d' % n_rows_times_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Duplication and Nullness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_ratio_duplication_noncateg_features=10.862000\n",
      "ratio_features_at_least_one_null=0.000000\n",
      "avg_ratio_null_values_features=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Average ratio of duplication across non-categorical features\n",
    "avg_ratio_duplication_noncateg_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['categorical'] is False:\n",
    "        avg_ratio_duplication_noncateg_features += feature_stats['statistics']['unique_ratio']\n",
    "avg_ratio_duplication_noncateg_features /= n_categorical_features\n",
    "\n",
    "# Ratio of features that contain at least one null value\n",
    "ratio_features_at_least_one_null = global_stats['row_has_null_ratio']\n",
    "\n",
    "# Average ratio of null values across features\n",
    "avg_ratio_null_values_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    avg_ratio_null_values_features += feature_stats['statistics']['null_count'] / n_rows\n",
    "avg_ratio_null_values_features /= n_features\n",
    "\n",
    "print('avg_ratio_duplication_noncateg_features=%f' % avg_ratio_duplication_noncateg_features)\n",
    "print('ratio_features_at_least_one_null=%f' % ratio_features_at_least_one_null)\n",
    "print('avg_ratio_null_values_features=%f' % avg_ratio_null_values_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_variance_numerical_features=80.732275\n",
      "avg_skewness_numerical_features=1.805717\n",
      "avg_kurtosis_numerical_features=4.453533\n",
      "avg_median_dev_numerical_features=3.152125\n",
      "avg_n_outliers_numerical_features=1.000000\n",
      "avg_unalikeability_categ_features=0.716700\n"
     ]
    }
   ],
   "source": [
    "# # # # # 1. Numerical features\n",
    "numerical_types = ['integer', 'float']\n",
    "n_numerical_features = n_int_features + n_float_features\n",
    "# Average variance across features for int, float (numerical) data types\n",
    "avg_variance_numerical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] in numerical_types:\n",
    "        avg_variance_numerical_features += feature_stats['statistics']['variance']\n",
    "avg_variance_numerical_features /= n_numerical_features\n",
    "\n",
    "# Average skewness across features for int, float (numerical) data types\n",
    "avg_skewness_numerical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] in numerical_types:\n",
    "        avg_skewness_numerical_features += feature_stats['statistics']['skewness']\n",
    "avg_skewness_numerical_features /= n_numerical_features\n",
    "\n",
    "# Average kurtosis across features for int, float (numerical) data types\n",
    "avg_kurtosis_numerical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] in numerical_types:\n",
    "        avg_kurtosis_numerical_features += feature_stats['statistics']['kurtosis']\n",
    "avg_kurtosis_numerical_features /= n_numerical_features\n",
    "\n",
    "# Average median absolute deviation across features for int, float (numerical) data types\n",
    "avg_median_dev_numerical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] in numerical_types:\n",
    "        avg_median_dev_numerical_features += feature_stats['statistics']['median_abs_deviation']\n",
    "avg_median_dev_numerical_features /= n_numerical_features\n",
    "\n",
    "# Average number of outliers across numerical features\n",
    "avg_n_outliers_numerical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] in numerical_types:\n",
    "        q1 = feature_stats['statistics']['quantiles'][0]\n",
    "        q2 = feature_stats['statistics']['quantiles'][1]\n",
    "        q3 = feature_stats['statistics']['quantiles'][2]\n",
    "        iqr = q3 - q1\n",
    "        lav = q1 - 1.5 * iqr\n",
    "        uav = q3 + 1.5 * iqr\n",
    "\n",
    "        data_type = 'int64'\n",
    "        if feature_stats['data_type'] == 'float':\n",
    "            data_type = 'float64'\n",
    "        \n",
    "        try:\n",
    "            df_column = data.data[feature_stats['column_name']].to_numpy(dtype=data_type, na_value=q2)\n",
    "        except Exception as e:\n",
    "            # TODO: exception message to include ref and column name\n",
    "            print(e)\n",
    "        avg_n_outliers_numerical_features += len(np.where((df_column > uav) & (df_column < lav)))\n",
    "avg_n_outliers_numerical_features /= n_numerical_features\n",
    "\n",
    "# # # # # 2. Categorical features\n",
    "# Average unalikeability impurity across categorical features\n",
    "avg_unalikeability_categ_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['categorical'] is True:\n",
    "        avg_unalikeability_categ_features += feature_stats['statistics']['unalikeability']\n",
    "avg_unalikeability_categ_features /= n_categorical_features\n",
    "\n",
    "\n",
    "print('avg_variance_numerical_features=%f' % avg_variance_numerical_features)\n",
    "print('avg_skewness_numerical_features=%f' % avg_skewness_numerical_features)\n",
    "print('avg_kurtosis_numerical_features=%f' % avg_kurtosis_numerical_features)\n",
    "print('avg_median_dev_numerical_features=%f' % avg_median_dev_numerical_features)\n",
    "print('avg_n_outliers_numerical_features=%f' % avg_n_outliers_numerical_features)\n",
    "print('avg_unalikeability_categ_features=%f' % avg_unalikeability_categ_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_margin_error_numerical_features=0.801867\n",
      "avg_gini_impurity_categ_features=0.692000\n"
     ]
    }
   ],
   "source": [
    "# Precision - average margin of error for numerical (int, float) features\n",
    "avg_margin_error_numerical_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['data_type'] in numerical_types:\n",
    "        avg_margin_error_numerical_features += feature_stats['statistics']['precision']['margin_of_error']\n",
    "avg_margin_error_numerical_features /= n_numerical_features\n",
    "\n",
    "# Average Gini impurity across categorical features\n",
    "avg_gini_impurity_categ_features = 0\n",
    "for feature_stats in data_stats:\n",
    "    if feature_stats['categorical'] is True:\n",
    "        avg_gini_impurity_categ_features += feature_stats['statistics']['gini_impurity']\n",
    "avg_gini_impurity_categ_features /= n_categorical_features\n",
    "\n",
    "print('avg_margin_error_numerical_features=%f' % avg_margin_error_numerical_features)\n",
    "print('avg_gini_impurity_categ_features=%f' % avg_gini_impurity_categ_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between current time and last update time\n",
    "time_diff_current_and_last_update = time.time() - last_update_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2020, tm_mon=8, tm_mday=5, tm_hour=11, tm_min=57, tm_sec=10, tm_wday=2, tm_yday=218, tm_isdst=-1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.strptime('8/5/2020 11:57:10 AM', '%m/%d/%Y %H:%M:%S %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 1), ('b', 2)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a':1, 'b':2}\n",
    "d.items()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pii_entities=0.483000\n"
     ]
    }
   ],
   "source": [
    "# Aproximate num of PII entities\n",
    "pii_labels = ['ADDRESS', 'CREDIT_CARD', 'DRIVERS_LICENSE', 'EMAIL_ADDRESS', 'UUID', 'IPV4', 'IPV6', 'MAC_ADDRESS', 'PERSON', 'PHONE_NUMBER', 'SSN']\n",
    "num_pii_entities = 0\n",
    "for feature_stats in data_stats:\n",
    "    predicted_label_ratios = feature_stats['statistics']['data_label_representation']\n",
    "    for pii_label in pii_labels:\n",
    "        num_pii_entities += predicted_label_ratios[pii_label] * n_rows\n",
    "num_pii_entities\n",
    "print('num_pii_entities=%f' % num_pii_entities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving performance of Data Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9682539682539693"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 30\n",
    "num_processes = 3\n",
    "x * 1000 / 60 / 60 / (num_processes * 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataprofiler import Data, Profiler, ProfilerOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded datasets in 0.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small file: 30 rows, 14 cols\n",
    "small_file = \"datasets\\\\sourabhji_availabilty-of-groundwater-future-use\\\\Groundwater.csv\"\n",
    "# Medium file: 8100 rows, 20+ cols\n",
    "medium_file = \"datasets\\\\uciml_mushroom-classification\\\\mushrooms.csv\"\n",
    "# Large file: 340k rows, 8 cols\n",
    "large_file = \"datasets\\\\akhilv11_border-crossing-entry-data\\\\Border_Crossing_Entry_Data.csv\"\n",
    "\n",
    "# Load file (CSV should be automatically identified)\n",
    "start = time.time()\n",
    "data = Data(large_file)\n",
    "end = time.time()\n",
    "print('\\nLoaded datasets in %.2f\\n' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radue\\AppData\\Roaming\\Python\\Python39\\site-packages\\dataprofiler\\profilers\\profiler_options.py:529: UserWarning: StructuredOptions.text.numeric_stats: The numeric stats are completely disabled.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DataProfiler.profilers.profile_builder: Finding the Null values in the columns... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:04<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:DataProfiler.profilers.profile_builder: Calculating the statistics... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8/8 [00:17<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Profiler took 28.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Profilers options (disable unused features to reduce compute time)\n",
    "profile_options = ProfilerOptions()\n",
    "# # Structured options\n",
    "profile_options.structured_options.multiprocess.is_enabled = False\n",
    "profile_options.structured_options.chi2_homogeneity.is_enabled = False\n",
    "# # # Numerical (int, float)\n",
    "# # # # Integer\n",
    "profile_options.structured_options.int.min.is_enabled = False\n",
    "profile_options.structured_options.int.max.is_enabled = False\n",
    "profile_options.structured_options.int.num_zeros.is_enabled = False\n",
    "profile_options.structured_options.int.num_negatives.is_enabled = False\n",
    "# # # # Float\n",
    "profile_options.structured_options.float.min.is_enabled = False\n",
    "profile_options.structured_options.float.max.is_enabled = False\n",
    "profile_options.structured_options.float.num_zeros.is_enabled = False\n",
    "profile_options.structured_options.float.num_negatives.is_enabled = False\n",
    "# # # Text\n",
    "profile_options.structured_options.text.vocab.is_enabled = False\n",
    "profile_options.structured_options.text.min.is_enabled = False\n",
    "profile_options.structured_options.text.max.is_enabled = False\n",
    "profile_options.structured_options.text.mode.is_enabled = False\n",
    "profile_options.structured_options.text.median.is_enabled = False\n",
    "profile_options.structured_options.text.sum.is_enabled = False\n",
    "profile_options.structured_options.texta.variance.is_enabled = False\n",
    "profile_options.structured_options.text.skewness.is_enabled = False\n",
    "profile_options.structured_options.text.kurtosis.is_enabled = False\n",
    "profile_options.structured_options.text.histogram_and_quantiles.is_enabled = False\n",
    "profile_options.structured_options.text.median_abs_deviation.is_enabled = False\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "# Profile the dataset\n",
    "print(data.data.shape[0])\n",
    "profile = Profiler(data, samples_per_update=data.data.shape[0], options=profile_options)\n",
    "end = time.time()\n",
    "print('\\nProfiler took %.2f\\n' % (end - start))\n",
    "\n",
    "\n",
    "# Print the report\n",
    "# print(json.dumps(report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report took 0.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a report and use json to prettify.\n",
    "start = time.time()\n",
    "report  = profile.report(report_options={\"output_format\": \"serializable\"})\n",
    "end = time.time()\n",
    "print('Report took %.2f\\n' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_refs = np.load('selected_datasets_ref1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('selected_datasets_for_download_with_proxy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dipam7/student-grade-prediction',\n",
       "       'akhilv11/border-crossing-entry-data',\n",
       "       'floser/french-motor-claims-datasets-fremtpl2freq', ...,\n",
       "       'sandeep2812/covid19casestudyjohnshopkinsuniversitydaily',\n",
       "       'sampledemoproject/covid-study-impact-data',\n",
       "       'manjarinandimajumdar/essayscsv'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ref'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>2</td></tr><tr><td>5</td><td>7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌─────┬─────┐\n",
       "│ a   ┆ b   │\n",
       "│ --- ┆ --- │\n",
       "│ i64 ┆ i64 │\n",
       "╞═════╪═════╡\n",
       "│ 1   ┆ 2   │\n",
       "│ 5   ┆ 7   │\n",
       "└─────┴─────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.from_dicts([{'a': 1, 'b': 2}, {'a': 5, 'b': 7}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3][1:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
