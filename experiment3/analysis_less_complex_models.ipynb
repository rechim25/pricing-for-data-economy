{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "RESULTS_PATH = './training_results/kernel_ridge'\n",
    "\n",
    "VALID_SIZE = 2000\n",
    "\n",
    "TRAIN_SIZE = 5000\n",
    "TRAIN_CLASS_1_RATIO = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load and Preprocess** Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Encode Gender\n",
    "    enc = OneHotEncoder(handle_unknown='error', sparse_output=False)\n",
    "    gender_oh_encoded = enc.fit_transform(data['Gender'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    # Encode Vehicle_Age\n",
    "    enc = OrdinalEncoder(categories=[['< 1 Year', '1-2 Year', '> 2 Years']], handle_unknown='error')\n",
    "    vehicle_age_encoded = enc.fit_transform(data['Vehicle_Age'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    # Encode Vehicle_Damage\n",
    "    enc = OneHotEncoder(handle_unknown='error', sparse_output=False)\n",
    "    vehicle_damage_encoded = enc.fit_transform(data['Vehicle_Damage'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    # Standardize variables\n",
    "    scaler = StandardScaler()\n",
    "    age_standard = scaler.fit_transform(data['Age'].to_numpy().reshape(-1, 1))\n",
    "    annual_premium_standard = scaler.fit_transform(data['Annual_Premium'].to_numpy().reshape(-1, 1))\n",
    "    vintage_standard = scaler.fit_transform(data['Vintage'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    data = data.drop(['Age', 'Annual_Premium', 'Vintage', 'Gender', 'Vehicle_Age', 'Vehicle_Damage']).with_columns(\n",
    "        [\n",
    "            pl.Series('Female', values=gender_oh_encoded[:, 0]),\n",
    "            pl.Series('Male', values=gender_oh_encoded[:, 1]),\n",
    "            pl.Series('Age', values=age_standard[:, 0]),\n",
    "            pl.Series('Annual_Premium', values=annual_premium_standard[:, 0]),\n",
    "            pl.Series('Vintage', values=vintage_standard[:, 0]),\n",
    "            pl.Series('Vehicle_Age', values=vehicle_age_encoded[:, 0]),\n",
    "            pl.Series('No_vehicle_damage', values=vehicle_damage_encoded[:, 0]),\n",
    "            pl.Series('Vehicle_Damage', values=vehicle_damage_encoded[:, 1])\n",
    "        ]\n",
    "    )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_split(\n",
    "        data_only_0: pl.DataFrame,\n",
    "        data_only_1: pl.DataFrame,\n",
    "        size: int\n",
    ") -> tuple[pl.DataFrame]:\n",
    "    # Balance out dataset\n",
    "    num_y_1 = size * 50 // 100\n",
    "    num_y_0 = size - num_y_1\n",
    "    \n",
    "    # Select validation slices\n",
    "    data_only_1_valid = data_only_1[:num_y_1]\n",
    "    data_only_0_valid = data_only_0[:num_y_0]\n",
    "\n",
    "    # Remove validation slices from the whole set (avoid overlapping with training set)\n",
    "    data_only_1 = data_only_1[num_y_1:]\n",
    "    data_only_0 = data_only_0[num_y_0:]\n",
    "\n",
    "    data_valid = pl.concat(\n",
    "        [\n",
    "            data_only_1_valid,\n",
    "            data_only_0_valid\n",
    "        ],\n",
    "        how='vertical'\n",
    "    ).sample(frac=1, shuffle=True, seed=83409)\n",
    "\n",
    "    assert data_valid.filter(pl.col('Response') == 1).shape[0] == data_valid.filter(pl.col('Response') == 0).shape[0]\n",
    "\n",
    "    return data_valid, data_only_0, data_only_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_split(\n",
    "        data_only_0: pl.DataFrame, \n",
    "        data_only_1: pl.DataFrame, \n",
    "        size: int, \n",
    "        class_1_ratio: float,\n",
    "        seed: int\n",
    ") -> tuple[pl.DataFrame]:\n",
    "    num_y_1 = int(size * class_1_ratio)\n",
    "    num_y_0 = size - num_y_1\n",
    "\n",
    "    data_train = pl.concat(\n",
    "        [\n",
    "            df_y_1[:num_y_1],\n",
    "            df_y_0[:num_y_0]\n",
    "        ],\n",
    "        how='vertical'\n",
    "    ).sample(frac=1, shuffle=True, seed=seed)\n",
    "\n",
    "    # Remove training slices from the whole set (avoid future overlapping)\n",
    "    data_only_1 = data_only_1[num_y_1:]\n",
    "    data_only_0 = data_only_0[num_y_0:]\n",
    "\n",
    "    return data_train, data_only_0, data_only_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('./health-insurance-data/train.csv')\n",
    "df = df.sample(frac=1, shuffle=True, seed=68123)\n",
    "\n",
    "# Preprocess Dataset\n",
    "df = preprocess_dataset(data=df)\n",
    "\n",
    "# Split dataset based on the binary label, then shuffle\n",
    "df_y_0 = df.filter(pl.col('Response') == 0).sample(frac=1, shuffle=True, seed=11897)\n",
    "df_y_1 = df.filter(pl.col('Response') == 1).sample(frac=1, shuffle=True, seed=4199)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "num_y_1 = int(size * 0.5)\n",
    "num_y_0 = size - num_y_1\n",
    "\n",
    "data_train = pl.concat(\n",
    "    [\n",
    "        df_y_1[:num_y_1],\n",
    "        df_y_0[:num_y_0]\n",
    "    ],\n",
    "    how='vertical'\n",
    ").sample(frac=1, shuffle=True, seed=41212636)\n",
    "\n",
    "data_train.write_csv('train_deea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('train_deea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after preprocessing: ['id', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel', 'Response', 'Female', 'Male', 'Age', 'Annual_Premium', 'Vintage', 'Vehicle_Age', 'No_vehicle_damage', 'Vehicle_Damage']\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv('./health-insurance-data/train.csv')\n",
    "df = df.sample(frac=1, shuffle=True, seed=68123)\n",
    "\n",
    "# Preprocess Dataset\n",
    "df = preprocess_dataset(data=df)\n",
    "print(f'Columns after preprocessing: {df.columns}')\n",
    "\n",
    "# Split dataset based on the binary label, then shuffle\n",
    "df_y_0 = df.filter(pl.col('Response') == 0).sample(frac=1, shuffle=True, seed=11897)\n",
    "df_y_1 = df.filter(pl.col('Response') == 1).sample(frac=1, shuffle=True, seed=4199)\n",
    "\n",
    "valid_size = VALID_SIZE\n",
    "# Get validation split and remove it from dataset to avoid overlapping with training set\n",
    "df_valid, df_y_0, df_y_1 = get_validation_split(data_only_0=df_y_0, data_only_1=df_y_1, size=valid_size)\n",
    "\n",
    "train_size = TRAIN_SIZE\n",
    "# Select unbalanced training set\n",
    "df_train, df_y_0, df_y_1= get_training_split(\n",
    "    data_only_0=df_y_0,\n",
    "    data_only_1=df_y_1, \n",
    "    size=train_size, \n",
    "    class_1_ratio=TRAIN_CLASS_1_RATIO, \n",
    "    seed=4128211\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select matrices for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.select(pl.exclude(['id', 'Response'])).to_numpy()\n",
    "y_train =df_train['Response'].to_numpy().reshape(-1, 1)\n",
    "X_valid = df_valid.select(pl.exclude(['id', 'Response'])).to_numpy()\n",
    "y_valid = df_valid['Response'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 12), (5000, 1))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 12), (2000, 1))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, y_valid.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Regression** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRR Acc = 0.7855\n",
      "Time taken = 2.068s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time()\n",
    "\n",
    "krr = KernelRidge(alpha=1.0, kernel='linear')\n",
    "krr.fit(X_train, y_train)\n",
    "\n",
    "pred_values = krr.predict(X_valid)\n",
    "pred_labels = np.round(pred_values)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, pred_labels)\n",
    "print(f'LRR Acc = {accuracy:.4f}')\n",
    "print(f'Time taken = {(time() - start):.3f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernel Ridge Regression** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR Acc = 0.7695\n",
      "Time taken = 5.025s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "krr = KernelRidge(alpha=1.0, kernel='rbf')\n",
    "krr.fit(X_train, y_train)\n",
    "\n",
    "pred_values = krr.predict(X_valid)\n",
    "pred_labels = np.round(pred_values)\n",
    "\n",
    "accuracy = accuracy_score(y_valid, pred_labels)\n",
    "print(f'KRR Acc = {accuracy:.4f}')\n",
    "print(f'Time taken = {(time() - start):.3f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGBoost** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Acc = 0.7750\n",
      "Time taken = 0.482s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"binary:logistic\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "pred_values = xgb_model.predict(X_valid)\n",
    "pred_labels = np.round(pred_values)\n",
    "\n",
    "accuracy = accuracy = accuracy_score(y_valid, pred_labels)\n",
    "print(f'XGBoost Acc = {accuracy:.4f}')\n",
    "print(f'Time taken = {(time() - start):.3f}s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRR Acc = 0.7855\n",
      "Time taken = 2.780s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.301s\n",
      "XGBoost Acc = 0.7750\n",
      "Time taken = 0.464s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.098s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.018s\n",
      "XGBoost Acc = 0.7750\n",
      "Time taken = 0.463s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.367s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.842s\n",
      "XGBoost Acc = 0.7750\n",
      "Time taken = 0.777s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.702s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.522s\n",
      "XGBoost Acc = 0.7750\n",
      "Time taken = 0.530s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.451s\n",
      "LRR Acc = 0.7855\n",
      "Time taken = 2.506s\n",
      "XGBoost Acc = 0.7750\n",
      "Time taken = 0.534s\n"
     ]
    }
   ],
   "source": [
    "comp_time = {\n",
    "    'lrr': [],\n",
    "    'krr': [],\n",
    "    'xgb': []\n",
    "}\n",
    "for i in range(0, 5):\n",
    "    start = time()\n",
    "\n",
    "    # Linear Ridge\n",
    "    krr = KernelRidge(alpha=1.0, kernel='linear')\n",
    "    krr.fit(X_train, y_train)\n",
    "\n",
    "    pred_values = krr.predict(X_valid)\n",
    "    pred_labels = np.round(pred_values)\n",
    "\n",
    "    accuracy = accuracy_score(y_valid, pred_labels)\n",
    "    delta_time = time() - start\n",
    "    print(f'LRR Acc = {accuracy:.4f}')\n",
    "    print(f'Time taken = {delta_time:.3f}s')\n",
    "    comp_time['lrr'].append(delta_time)\n",
    "\n",
    "    # RBF Kernel Ridge\n",
    "    start = time()\n",
    "    krr = KernelRidge(alpha=1.0, kernel='linear')\n",
    "    krr.fit(X_train, y_train)\n",
    "    pred_values = krr.predict(X_valid)\n",
    "    pred_labels = np.round(pred_values)\n",
    "    accuracy = accuracy_score(y_valid, pred_labels)\n",
    "    delta_time = time() - start\n",
    "    print(f'LRR Acc = {accuracy:.4f}')\n",
    "    print(f'Time taken = {(time() - start):.3f}s')\n",
    "    comp_time['krr'].append(delta_time)\n",
    "\n",
    "    # XGBoost\n",
    "    start = time()\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"binary:logistic\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    pred_values = xgb_model.predict(X_valid)\n",
    "    pred_labels = np.round(pred_values)\n",
    "    accuracy = accuracy = accuracy_score(y_valid, pred_labels)\n",
    "    print(f'XGBoost Acc = {accuracy:.4f}')\n",
    "    print(f'Time taken = {(time() - start):.3f}s')\n",
    "    comp_time['xgb'].append(delta_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.479885149002075, 2.437330627441406, 2.437330627441406)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(comp_time['lrr']), np.mean(comp_time['krr']), np.mean(comp_time['xgb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
